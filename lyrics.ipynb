{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb\n",
    "### get the pos in order, what occurs\n",
    "### amd then also get words for those pos and fill in the blanks to make a new one\n",
    "\n",
    "\n",
    "### train with sequences of words and then guess the target next word\n",
    "\n",
    "### The idea is that after many epochs the RNN will learn “the style” of how the corpus is written, trying to adjust the weights of the network to predict the next word given a sequence of the N previous words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus, split into words\n",
    "### The first step is to read the corpus and split it into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sys.argv[1] # first command line arg\n",
    "with io.open(corpus, encoding='utf-8') as f:\n",
    "    text = f.read().lower().replace('\\n', ' \\n ')\n",
    "print('Corpus length in characters:', len(text))\n",
    "\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_in_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus, split into words\n",
    "### The first step is to read the corpus and split it into words.\n",
    "\n",
    "## Note the call to .replace(‘\\n’, ‘ \\n ‘, this is because we want the newline as a word. The idea behind this is that we are also leaving to the network the decision on when to start a new line (after several words). After this, text_in_words is a big array containing all the corpus, word by word.\n",
    "\n",
    "# https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "\n",
    "# https://medium.com/@enriqueav/update-automatic-song-lyrics-creator-with-word-embeddings-e30de94db8d1\n",
    "\n",
    "### A trained layer converts the words in the vocabulary in a way that the ones with similar use will be close in their vectorial representation. This means, for instance, that words representing colors: “white”, “red”, “yellow”, etc will be close among them.\n",
    "\n",
    "### https://github.com/ApurbaSengupta/Lyrics-Generation-using-BERT/blob/master/code/reconstruction.ipynb\n",
    "\n",
    "\n",
    "one idea is to take all of the sentences from a given genre and then use word2vec to find simmilar senteacnes and swamp them around \n",
    "https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction\n",
    "\n",
    "another is to switch out word for word, start at first word guess the second then third and so on (replace all words with simmilar words)\n",
    "\n",
    "another is to remove certain words and then replcae those with a guess based on the previous context\n",
    "\n",
    "https://datascience.stackexchange.com/questions/9785/predicting-a-word-using-word2vec-model\n",
    "\n",
    "use word2vec to o thro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method 1: keep sentance structure but compute new words for each using missing word "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
